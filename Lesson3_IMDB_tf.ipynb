{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в искусственные нейронные сети\n",
    "# Урок 3. TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронная сеть, которая может вычислять настроение авторов кинорецензий "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим датесет IMDb, который уже встроен в Keras. \n",
    "\n",
    "**Так как нам не подходят пропорции 50/50 данных обучения и тестирования, мы объединим их для \n",
    "последующего разделения в пропорции 80/20**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Изучение данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [0 1]\n",
      "Number of unique words: 9998\n"
     ]
    }
   ],
   "source": [
    "print(\"Categories:\", np.unique(targets))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review length: 234.75892\n",
      "Standard Deviation: 173.0\n"
     ]
    }
   ],
   "source": [
    "length = [len(i) for i in data]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**У нас имеются две категории: 0 или 1, что представляет собой настроение обзора. Весь датасет содержит 9998 уникальных слов, средний размер обзора составляет 234 слова со стандартным отклонением 173.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Label:\", targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"
     ]
    }
   ],
   "source": [
    "print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had # working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how # this is to watch save yourself an hour a bit of your life\n"
     ]
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[2]] )# обратное преобразование индексов в слова\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подготовка данных.**\n",
    "\n",
    "**Самый большой обзор имеет около 10 000 слов, поэтому все обзоры приведем к такому же количеству слов, \n",
    "заполнив недостающие символы нулями**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    " \n",
    "data = vectorize(data)\n",
    "targets = np.array(targets).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разделим датасет на обучающий и тестировочный наборы. Обучающий набор будет состоять из 40 000 обзоров, тестировочный — из 10 000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = data[:10000]\n",
    "test_y = targets[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = targets[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10000:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 70)                700070    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                3550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 703,671\n",
      "Trainable params: 703,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "  # Input Layer  \n",
    "  Dense(70, activation='elu', input_shape=(10000, )),\n",
    "  # Hidden Layers  \n",
    "  Dropout(0.7, noise_shape=None, seed=None),\n",
    "  Dense(50, activation='elu'),\n",
    "#   Dropout(0.5, noise_shape=None, seed=None),\n",
    "#   Dense(170, activation='elu'),  \n",
    "#   Dropout(0.5, noise_shape=None, seed=None),\n",
    "#   Dense(120, activation='elu'),     \n",
    "  # Output Layer, softmax тут не работает, использовал sigmoid\n",
    "  # Dense(1, activation='softmax'), \n",
    "  Dense(1, activation='sigmoid'), \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 4s 105us/step - loss: 0.3654 - accuracy: 0.8410 - val_loss: 0.2604 - val_accuracy: 0.8964\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 4s 97us/step - loss: 0.2240 - accuracy: 0.9130 - val_loss: 0.2623 - val_accuracy: 0.8974\n",
      "Test-Accuracy: 0.8968999981880188\n"
     ]
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")\n",
    "# Fit the model\n",
    "results = model.fit(\n",
    " train_x, train_y,\n",
    " epochs= 2,\n",
    " batch_size = 500,\n",
    " validation_data = (test_x, test_y)\n",
    ")\n",
    "print(\"Test-Accuracy:\", np.mean(results.history[\"val_accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нейронная сеть получилась с четырьмя слоями. В скрытых слоях использована функция активации «elu». На каждом слое используется функция «dense» для полносвязного соединения слоев друг с другом. Использован оптимизатор «adam». Оптимизатор — это алгоритм, который изменяет веса и смещения во время обучения. В качестве функции потерь используем бинарную кросс-энтропию (так как мы работаем с бинарной классификацией), в качестве метрики оценки — точность.**\n",
    "\n",
    "Модель можно использовать для анализа бинарных настроений в других источниках, но для этого придется сделать их размер равным 10 000 или изменить параметры входного слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
